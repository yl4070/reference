
@article{maennel_what_2020,
	title = {What {Do} {Neural} {Networks} {Learn} {When} {Trained} {With} {Random} {Labels}},
	volume = {33},
	abstract = {We study deep neural networks (DNNs) trained on natural image data with entirely random labels. Despite its popularity in the literature, where it is often used to study memorization, generalization, and other phenomena, little is known about what DNNs learn in this setting. In this paper, we show analytically for convolutional and fully connected networks that an alignment between the principal components of network parameters and data takes place when training with random labels. We study this alignment effect by investigating neural networks pre-trained on randomly labelled image data and subsequently fine-tuned on disjoint datasets with random or real labels. We show how this alignment produces a positive transfer: networks pre-trained with random labels train faster downstream compared to training from scratch even after accounting for simple effects, such as weight scaling. We analyze how competing effects, such as specialization at later layers, may hide the positive transfer. These effects are studied in several network architectures, including VGG16 and ResNet18, on CIFAR10 and ImageNet.},
	journal = {Neural Information Processing Systems},
	author = {Maennel, Hartmut and Alabdulmohsin, Ibrahim M. and Tolstikhin, Ilya and Baldock, Robert John Nicholas and Bousquet, Olivier and Gelly, Sylvain and Keysers, Daniel},
	year = {2020},
	note = {ARXIV\_ID: 2006.10455
MAG ID: 3100945695
S2ID: 50d5e1d2cda879ee07a024d47b04d5f07b689575},
	pages = {19693--19704},
}
